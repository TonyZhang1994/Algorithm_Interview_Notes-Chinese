{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSCI544_PyTorchDL_Tutorial_Todo.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TonyZhang1994/Algorithm_Interview_Notes-Chinese/blob/master/CSCI544_PyTorchDL_Tutorial_Todo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mlc3xG6EAPMp",
        "colab_type": "text"
      },
      "source": [
        "Author: Rujun (RJ) Han \\\\\n",
        "Created: September 2019 \\\\\n",
        "2nd Part adapted from TG's tutorial: https://colab.research.google.com/drive/10WICfaAQcXKDM8P07JuoeQL01pTx64TB "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIn5d3K7E1lo",
        "colab_type": "text"
      },
      "source": [
        "# Objective\n",
        "\n",
        "1.   Hands-on PyTorch Tutorial\n",
        "2.   Demonstrate PyTorch end-to-end framework to build deep learning pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_4o-1q7IHJs",
        "colab_type": "text"
      },
      "source": [
        "# **Overview**\n",
        "\n",
        "*   Colab Setup\n",
        "*   PyTorch Fundamentals + Feed Forward Network\n",
        "*   Task: Sentiment Analysis on Movie Reviews dataset\n",
        "*   Data Preparation\n",
        "*   Trainer\n",
        "*   Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkAlIVoLEHyq",
        "colab_type": "code",
        "outputId": "f28443d5-208e-4fd4-ae36-9a98513b0f8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"NLP is awesome!\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLP is awesome!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL3o4eTsLrZm",
        "colab_type": "text"
      },
      "source": [
        "# Colab Setup\n",
        "\n",
        "### Enable GPU backend\n",
        "Edit > Notebook Settings > Hardware Accelerator = GPU \\\\\n",
        "\n",
        "### Locate where this notebook is saved\n",
        "By default, it is saved in Google Drive. File > Locate in Drive \\\\\n",
        "\n",
        "Summary: Colab is awesome (no setup required!) Thanks Google"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BlqnaKMMKg_",
        "colab_type": "text"
      },
      "source": [
        "# I. PyTorch Fundamentals\n",
        "\n",
        "**Offical Website:** https://pytorch.org \\\\\n",
        "An open source machine learning framework that accelerates the path from research prototyping to production deployment.\n",
        "\n",
        "**Popular Alternatives:** Tensorflow, Keras, Theano, etc.\n",
        "\n",
        "**PyTorch vs. Tensorflow** \n",
        "[Dynamic vs. Static computation graph](https://medium.com/@omaraymanomar/dynamic-vs-static-computation-graph-2579d1934ecf) (FAIR vs. Google AI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeMT2nksj5MK",
        "colab_type": "text"
      },
      "source": [
        "## Practice I: Manipulating Tensors\n",
        "Special case: Vector (1D); Matrix (2D)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAna38---pf-",
        "colab_type": "code",
        "outputId": "ff678472-5061-4aa9-f1d6-9d1516749d79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import torch\n",
        "## Resources\n",
        "print('Using Torch', torch.__version__)\n",
        "print('GPU available?', torch.cuda.is_available())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using Torch 1.1.0\n",
            "GPU available? True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxGdSyq-bRR4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.#tensor([0.1, 0.2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3umc45S8kquk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dtype = torch.float\n",
        "device = torch.device(\"cpu\")\n",
        "#device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
        "\n",
        "# N is (data) batch size; D_in is input dimension; \n",
        "# H is hidden dimension; D_out is output dimension.\n",
        "\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10 # 64 * 1000 --> 64 * 10\n",
        "\n",
        "# Create random input and output data\n",
        "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
        "\n",
        "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
        "\n",
        "# Randomly initialize weights\n",
        "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
        "w2 = torch.randn(H, D_out, device=device, dtype=dtype)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e26mPTv-Mh_y",
        "colab_type": "code",
        "outputId": "ac1d5a5f-59aa-45f0-c5b0-08392956c453",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "x.shape, y.shape, w1.shape, w2.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([64, 1000]),\n",
              " torch.Size([64, 10]),\n",
              " torch.Size([1000, 100]),\n",
              " torch.Size([100, 10]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BbOxW5lMgSR",
        "colab_type": "code",
        "outputId": "01ea2545-3f9f-4532-f058-1f8bd3c78318",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# matrix multiplication: X * W1 -- (64 * 1000) x (1000 * 100)\n",
        "h1 = x.mm(w1)\n",
        "h1.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 100])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U--pYGBoWRh",
        "colab_type": "text"
      },
      "source": [
        "### Todo (5 - 10 mins): \n",
        "Create a random tensor **b** with dimenson N * 1 \\\\\n",
        "Update $h_1$ by adding **b** \\\\\n",
        "Print shape of this tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0C6Wg-gMq15S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "St4M_3O1qs7U",
        "colab_type": "text"
      },
      "source": [
        "## Practice II: Activation Functions\n",
        "\n",
        "Neural Network: [activation functions](https://en.wikipedia.org/wiki/Activation_function)\n",
        "\n",
        "**Why do we need activation functions**?\n",
        "\n",
        "\n",
        "1.   Non-linearity -- allows NN to approximate arbitrary functions\n",
        "2.   Mapping output into probability space (softmax function)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2sfl2t5laq4",
        "colab_type": "code",
        "outputId": "372ff460-ba36-4bda-9bfc-74f316f8b802",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "torch.relu(x), x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[2.3810, 0.0000, 0.0330,  ..., 0.0000, 2.0773, 0.4446],\n",
              "         [0.3052, 0.0791, 0.0000,  ..., 0.2019, 1.5184, 0.0000],\n",
              "         [0.0000, 0.0000, 0.0000,  ..., 0.4033, 0.0000, 0.3911],\n",
              "         ...,\n",
              "         [0.0000, 0.0485, 0.0000,  ..., 1.1079, 0.3335, 0.9327],\n",
              "         [0.0000, 2.1679, 0.0000,  ..., 0.0998, 0.0000, 0.4320],\n",
              "         [0.6522, 0.6501, 0.0000,  ..., 0.0000, 0.0000, 1.0330]]),\n",
              " tensor([[ 2.3810, -1.1657,  0.0330,  ..., -1.6808,  2.0773,  0.4446],\n",
              "         [ 0.3052,  0.0791, -1.1296,  ...,  0.2019,  1.5184, -0.7700],\n",
              "         [-3.1773, -0.3608, -1.4235,  ...,  0.4033, -0.2724,  0.3911],\n",
              "         ...,\n",
              "         [-0.2724,  0.0485, -0.4152,  ...,  1.1079,  0.3335,  0.9327],\n",
              "         [-0.2812,  2.1679, -0.1425,  ...,  0.0998, -0.4558,  0.4320],\n",
              "         [ 0.6522,  0.6501, -1.5359,  ..., -1.9835, -2.1921,  1.0330]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x72-PtBnwJCg",
        "colab_type": "text"
      },
      "source": [
        "### Todo (2 mins):\n",
        "*   Update your previous previous calculation with sigmoid function.\n",
        "*   Alternatively, you may try torch.relu(), torch.tanh(), etc.\n",
        "*   Name the result as **h_activated**, output its dimension"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z897rzvYwNJD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3u89Jvr64h66",
        "colab_type": "text"
      },
      "source": [
        "## Practice III: Loss Function and Supervised Learning\n",
        "\n",
        "Supervised Learing vs. Unsupervised Learning? \\\\\n",
        "\n",
        "Square Loss: $\\sum_i^N (y - y_{pred})^2$ \\\\\n",
        "\n",
        "Can we use $h_{activated}$ as $y_{pred}$, why and why not? \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVANdhgg937Q",
        "colab_type": "text"
      },
      "source": [
        "### Todo (5 mins):\n",
        "calculate $y_{pred} = h_{activated} * w_2 + c$, constant c is optional"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQfchysP0G_S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### your code here\n",
        "\n",
        "#loss = (y_pred - y).pow(2).sum().item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MQvZ424BPme",
        "colab_type": "text"
      },
      "source": [
        "## What's next?\n",
        "We've built a very simple DL model architecture -- Feed Forward Neural Network. But how to learn parameters, i.e. $w_1, w_2, b, c$? \n",
        "*   [Backpropagation](https://brilliant.org/wiki/backpropagation/)\n",
        "*   Iterate until convergence -- gradient descent\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8j141CR288yv",
        "colab_type": "text"
      },
      "source": [
        "### Wrap everything up with PyTorch [NN modules](https://pytorch.org/docs/stable/nn.html). Search for torch.nn.linear."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akzbXpTj7U06",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FFN(torch.nn.Module):\n",
        "    def __init__(self, D_in, H, D_out):\n",
        "        \"\"\"\n",
        "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
        "        member variables.\n",
        "        \"\"\"\n",
        "        super(FFN, self).__init__()\n",
        "        self.linear1 = torch.nn.Linear(D_in, H)\n",
        "        self.linear2 = torch.nn.Linear(H, D_out)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        In the forward function we accept a Tensor of input data and we must return\n",
        "        a Tensor of output data. We can use Modules defined in the constructor as\n",
        "        well as arbitrary operators on Tensors.\n",
        "        \"\"\"\n",
        "        h_activated = self.linear1(x).sigmoid()\n",
        "        y_pred = self.linear2(h_activated)\n",
        "        return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Wd1GRG0Q4A7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = FFN(D_in, H, D_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgT8Cr6J-I6d",
        "colab_type": "text"
      },
      "source": [
        "### Another (much more convenient) way to create loss function in PyTorch. \n",
        "\n",
        "PyTorch package contains most commonly used loss functions. But we can easily create our own and integrate it with PyTorch pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHdf1Szk7iUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_fn = torch.nn.MSELoss(reduction='sum')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oeva-qqgZTt",
        "colab_type": "text"
      },
      "source": [
        "### Questions?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVfRmkr8AB7x",
        "colab_type": "text"
      },
      "source": [
        "### PyTorch [Optimizers](https://pytorch.org/docs/stable/optim.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcVFsomE__-4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "learning_rate = 1e-4\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOmT7U2sCfr7",
        "colab_type": "text"
      },
      "source": [
        "### Parameter learning through iterative algorithm. [Gradient Descent](https://en.wikipedia.org/wiki/Gradient_descent)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4hWqDYK6bd6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for t in range(10):\n",
        "    # Forward pass: compute predicted y by passing x to the model.\n",
        "    y_pred = # fill in the blank here\n",
        "\n",
        "    # Compute and print loss.\n",
        "    loss = loss_fn(,) # fill in the blank here\n",
        "    \n",
        "    print(t, loss.item())\n",
        "\n",
        "    # Before the backward pass, use the optimizer object to zero all of the\n",
        "    # gradients for the variables it will update (which are the learnable\n",
        "    # weights of the model). This is because by default, gradients are\n",
        "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
        "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Backward pass: compute gradient of the loss with respect to model\n",
        "    # parameters\n",
        "    loss.backward()\n",
        "\n",
        "    # Calling the step function on an Optimizer makes an update to its\n",
        "    # parameters: w_1(t) = w_1(t-1) - lr * gradient(w_1)\n",
        "    optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iD9KrWnyDHpe",
        "colab_type": "text"
      },
      "source": [
        "# II. Sentiment Analysis on Movie Reviews dataset \n",
        "\n",
        "### We will move very fast in this section; the goal is to demonstrate an end-to-end PyTorch pipeline for HW and project.\n",
        "\n",
        "We want to build a non-linear (binary) classfier on text data.\n",
        "\n",
        "Large Movie Review Dataset from https://ai.stanford.edu/~amaas/data/sentiment/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6DHIjyrF2Xc",
        "colab_type": "text"
      },
      "source": [
        "## Download data and tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_L87W64m-sw8",
        "colab_type": "code",
        "outputId": "1c95d9d4-1f31-4185-ed95-ba3f4e6c2dfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        }
      },
      "source": [
        "![[ -f aclImdb_v1.tar.gz ]]  && echo \"Data already Downloaded\" || wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "![[ -d aclImdb ]] && echo \"already extracted\" || tar xf aclImdb_v1.tar.gz \n",
        "# Install Tokenizer\n",
        "! pip install mosestokenizer"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-09-18 04:07:42--  https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  20.7MB/s    in 6.6s    \n",
            "\n",
            "2019-09-18 04:07:49 (12.1 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n",
            "Collecting mosestokenizer\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/c6/913c968e5cbcaff6cdd2a54a1008330c01a573ecadcdf9f526058e3d33a0/mosestokenizer-1.0.0-py3-none-any.whl (51kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 2.1MB/s \n",
            "\u001b[?25hCollecting openfile (from mosestokenizer)\n",
            "  Downloading https://files.pythonhosted.org/packages/93/e6/805db6867faacb488b44ba8e0829ef4de151dd0499f3c5da5f4ad11698a7/openfile-0.0.7-py3-none-any.whl\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.6/dist-packages (from mosestokenizer) (0.6.2)\n",
            "Collecting toolwrapper (from mosestokenizer)\n",
            "  Downloading https://files.pythonhosted.org/packages/ad/00/dba43b705ecb0d286576e78fc5b5d75c27ba3d1c4b80cf9a047ec3c6ad3f/toolwrapper-1.0.0.tar.gz\n",
            "Building wheels for collected packages: toolwrapper\n",
            "  Building wheel for toolwrapper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for toolwrapper: filename=toolwrapper-1.0.0-cp36-none-any.whl size=3225 sha256=e40587b5a38ae8d9221047a89daf92dff4f42774987d4cdc0a1c913c2060569b\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/b0/2e/b8c414550c8372586ebaab634cf1f93733349cfe2b1d694fe8\n",
            "Successfully built toolwrapper\n",
            "Installing collected packages: openfile, toolwrapper, mosestokenizer\n",
            "Successfully installed mosestokenizer-1.0.0 openfile-0.0.7 toolwrapper-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1XfuUZMDIhi",
        "colab_type": "code",
        "outputId": "b6c6561d-c9f5-4540-abbf-1c2ffdfb7dcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "aclImdb  aclImdb_v1.tar.gz  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1DI2OuiGAUF",
        "colab_type": "text"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Qhhy7mKGdaH",
        "colab_type": "text"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t518pszPDZG0",
        "colab_type": "code",
        "outputId": "9bb05242-f0ec-40da-fc4b-8970b4167492",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "from pathlib import Path\n",
        "from tqdm import tqdm \n",
        "from mosestokenizer import MosesTokenizer # SpaCy; NLTK; Wordpiece \n",
        "import logging as log\n",
        "\n",
        "log.basicConfig(level=log.INFO)\n",
        "tokr = MosesTokenizer()\n",
        "\n",
        "\n",
        "def read_all(dir):\n",
        "  \"\"\"Reads a dataset of pos and neg text\"\"\"\n",
        "  for p in dir.glob('pos/*.txt'):\n",
        "    yield ('POS', p.read_text())\n",
        "  for p in dir.glob('neg/*.txt'):\n",
        "    yield ('NEG', p.read_text())\n",
        "\n",
        "def read_tokenized(dir):\n",
        "  \"\"\"Tokenization wrapper\"\"\"\n",
        "  for label, text in read_all(dir):\n",
        "    yield (label, tokr(text))\n",
        "\n",
        "train_ful_file = Path('train.ful.tok.tsv')\n",
        "test_file = Path('test.tok.tsv')\n",
        "\n",
        "if not train_ful_file.exists():\n",
        "  log.info(\"creating train file\")\n",
        "  with train_ful_file.open('w') as w:\n",
        "    for label, toks in tqdm(read_tokenized(Path('aclImdb/train'))):\n",
        "      w.write(f'{label}\\t{\" \".join(toks)}\\n')\n",
        "\n",
        "if not test_file.exists():\n",
        "  log.info(\"creating test file\")\n",
        "  with test_file.open('w') as w:\n",
        "    for label, toks in tqdm(read_tokenized(Path('aclImdb/test'))):\n",
        "      w.write(f'{label}\\t{\" \".join(toks)}\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:mosestokenizer.tokenizer.MosesTokenizer:executing argv ['perl', '/usr/local/lib/python3.6/dist-packages/mosestokenizer/tokenizer-v1.1.perl', '-q', '-l', 'en', '-b', '-a']\n",
            "INFO:mosestokenizer.tokenizer.MosesTokenizer:spawned process 843\n",
            "INFO:root:creating train file\n",
            "25000it [00:24, 1017.98it/s]\n",
            "INFO:root:creating test file\n",
            "25000it [00:23, 1003.59it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0yRDo99DhOj",
        "colab_type": "code",
        "outputId": "65dc701c-1b53-4ffb-f698-d8cd86449ab4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokr(\"I'd like to visit united-nations\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', '&apos;d', 'like', 'to', 'visit', 'united', '@-@', 'nations']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKZUDfZfGGVH",
        "colab_type": "text"
      },
      "source": [
        "### Check Data and Sample Count"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VX_gFYuD4jG",
        "colab_type": "code",
        "outputId": "69ea8671-82fc-4be6-ae4f-95a2ab74afda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "# Look at the stats  - sanity check\n",
        "! wc -lw *.tsv\n",
        "! head -2 *.tsv \n",
        "! tail -n 2 *.tsv \n",
        "! echo \"Training\"\n",
        "! cut -f1 train.ful.tok.tsv | sort | uniq -c\n",
        "! echo \"Test\"\n",
        "! cut -f1 test.tok.tsv | sort | uniq -c"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   25000  7064505 test.tok.tsv\n",
            "   25000  7226208 train.ful.tok.tsv\n",
            "   50000 14290713 total\n",
            "==> test.tok.tsv <==\n",
            "POS\tThe Academy Award winning &apos; Kramer vs. Kramer &apos; follows a snazzy businessman Ted Kramer ( Dustin Hoffman ) and his divorce with his bored wife ( Meryl Streep ) . One day Ted &apos;s wife leaves him and their child in search for a better life , forcing Ted to become closer to his son ( Justin Henry ) . The two bond and become very close ( but only after some friction ) , and just as everything is going perfect Ted &apos;s wife comes back to town and wants sole custody of their son . Ted then goes on a mission not to let his son get taken away from him , and fights his wife in court . Dustin Hoffman gives a tremendous performance along with Meryl Streep , and young Justin Henry is impressive . It &apos;s a sad emotional roller @-@ coaster of a movie , but it &apos;s a very well @-@ made and inspiring film . The film took the Oscar for Best Picture at the 1979 Academy Awards , along with Best Actor for Hoffman and Best Supporting Actress for Streep . If you don &apos;t mind a tearjerker , &apos; Kramer vs. Kramer &apos; is a great film to watch . Grade : B +\n",
            "POS\tmondovino is a moving and rewarding documentary. in the world of wine there is a huge different between the big winery and the small one. it &apos;s not just about size of of your vineyard but also the amount of money and power you have. if you have enough money to place ads in the wine spectator and hire a so called &quot; wine except &quot; then it doesn &apos;t matter the size of your estate. also in business world of today wine often has to mass marketed and suited to people &apos;s taste. what is means many times wine filtered of it &apos;s origin. mondovino shows the commercial side of wine in that of mega producer Robert mondavi , and Michael Rolland the wine expert who shapes wine to the taste of today &apos;s critics like Robert parker who is also in the film. now these men are not evil or wrong for they have done a great deal of good for wine. but they have power on a grand scale. as we all know power corrupts. mondovino also shows small wine makers such as Aime Guilbert of the languedoc and Hubert de montille of volnay in burgundy. these wine makers are not starving wine makers but they know like all great wine makers that it &apos;s about where the grapes are from. the best example of this is explained not by a wine maker but by a Haitian man working for Neal rosenthal the wine importer. the area the grapes are grown the terroir that matters , that a guiding hand that knows this makes important real wine .\n",
            "\n",
            "==> train.ful.tok.tsv <==\n",
            "POS\tI enjoyed this film far more than anything had led me to anticipate ; from reading other comments here , I suspect it benefits enormously from being seen on a full @-@ size screen in the cinema , in the company of a cheerful and enthusiastic audience . I was lucky enough to have that experience , borne up on ripples of laughter from all around , and had an immensely good time with this undemanding comedy . &lt; br / &gt; &lt; br / &gt; For it is as a comedy that it shines , if it shines anywhere at all . The music is nothing special -- in fact , I hadn &apos;t realised it * was * a musical , and was very surprised when the assembled ancestors burst into half @-@ spoken lyric -- but I do have to admit that the half @-@ threat , half @-@ promise of &apos; Oh , what I &apos;ll do ... &apos; has proved far more catchy than it ever seemed at the time , as it &apos;s still going round and round in my head ! &lt; br / &gt; &lt; br / &gt; The plot , such as it is , largely pivots around the past history of the eponymous Francesca , a sixteenth @-@ century portrait sporting a distinctly anachronistic hairstyle and fur @-@ coat . Her idea on the sanctity of marriage don &apos;t quite jibe with those of her distant descendant , the Countess Angelina , and one can almost hear the storyline creaking at the seams under the strain of the Production Code in order to ensure that the heroine arrives unsullied in her much @-@ delayed marriage @-@ bed with the right man ... &lt; br / &gt; &lt; br / &gt; The romance is scarcely earth @-@ shattering , and in fact the first few scenes , played pretty well straight , verge on the tedious . But where script and film really come to life is in the battle of the sexes that follows . The impudence of Douglas Fairbanks Jr &apos;s courtship of Betty Grable &apos;s married Angelina is equalled only by Betty @-@ Grable @-@ as @-@ Francesca &apos;s pursuit of him in turn , culminating in complete role @-@ reversal in the hilarious fantasy sequence where she -- literally -- sweeps him off his feet . This is probably the comic climax of the plot , although the consequences of the Colonel &apos;s understandable confusion are worked out with a deft touch in the remaining two &apos; acts &apos; of the operetta @-@ structure , and the spectacle of Fairbanks &apos; blissful , bemused awakening is more or less worth the price of admission on its own . &lt; br / &gt; &lt; br / &gt; Grable is entirely convincing in establishing her two contrasting characters , wisely gets almost all the ( limited ) singing opportunities , and shares the honours where the swathes of quotable dialogue in the various verbal duels are concerned . But in the field of unspoken reaction she is really outclassed by her male supporting leads ; Fairbanks in particular is an absolute treat in a number of wordless sequences whose set @-@ up and humour is worthy of the silent screen . &lt; br / &gt; &lt; br / &gt; This film is too uneven in style to be a classic , varying from sparkling repartee to hackneyed tedium . But at its best it is quite honestly very funny indeed , and brought a round of spontaneous applause and laughter across the auditorium at the end as the lights went up . Out of tune with its times , it may have failed to draw contemporary audiences -- but , on this showing , really didn &apos;t deserve to be disowned by both Grable and Preminger , the ( uncredited ) director . This is no masterpiece , but a thoroughly entertaining minor work , and I for one found myself grinning in remembrance all the way home .\n",
            "POS\tWriter / Director / Co @-@ Star Adam Jones is headed for great things . That is the thought I had after seeing his feature film &quot; Cross Eyed &quot; . Rarely does an independent film leave me feeling as good as his did . Cleverly written and masterfully directed , &quot; Cross Eyed &quot; keeps you involved from beginning to end . Adam Jones may not be a well known name yet , but he will be . If this movie had one or two &quot; Named Actors &quot; it would be a Box Office sensation . I think it still has a chance to get seen by a main stream audience if just one film distributor takes the time to work this movie . Regardless of where it ends up , if you get a chance to see it you won &apos;t be disappointed .\n",
            "==> test.tok.tsv <==\n",
            "NEG\tPun intended . This low budget action / horror vehicle for Don Wilson &apos;s ability to kick things is the stuff direct @-@ to @-@ video fare is made of . &lt; br / &gt; &lt; br / &gt; The plot : Wilson is a humorless vampire hunter who comes under fire from local law enforcement after he is forced to slaughter creatures of the night in view of the public . Police chase him , vampires chase him , he responds by kicking ... a lot . &lt; br / &gt; &lt; br / &gt; There is a little more to the story , but it is so inconsequential that I honestly can &apos;t remember it ; I think people actually spoke in the movie , but that &apos;s up to debate . The plot is nothing more than a set @-@ up for Wilson to kill as many vampires as possible in the running time , usually by kicking them . &lt; br / &gt; &lt; br / &gt; The technical specs are , in a word , anemic . Little color treating , amateurish use of lighting , simplistic use of camera and angle . Blood and gore is noticeably limited , odd for this type of film . The most hurtful of the filming foul @-@ ups is the jarring shift to super @-@ shaky cam during each and every fight scene . If the camera begins to bounce around like a reese monkey on speed , then you know Don is about to start kicking everything in sight . &lt; br / &gt; &lt; br / &gt; All in all , this is the kind of bad movie that can be made good with a few friends and a lot of cynical humor . Otherwise , do not watch , unless you really like to watch things get kicked . &lt; br / &gt; &lt; br / &gt; 3 / 10\n",
            "NEG\tThe sole reason for someone wanting to see this film would be because of John Leguizamo . I remember the previews , and it looked to be another second rate comedy . But the fact that Mr. Leguizamo starred , tried to redeem it . His name , how known it was at the time or not , tried to sell it . &lt; br / &gt; &lt; br / &gt; I was pretty disappointed with the performance of Leguizamo . His days on &quot; House of Buggin &apos; &quot; ( an &quot; In Living Colour &quot; clone ) , were his tip @-@ top . There is a fine line between wackiness and idiocy , and we &apos;ll just say that Leguizamo crossed it tenfold . He looked like he was trying to be too outrageous and crazy for the camera . As a matter of fact , I &apos;ll say that he tried too hard . Madcap humor spilled over into stupidity , and the film was spoiled . I can &apos;t say I blamed him , if you were given this opportunity , you &apos;d try as hard if you could , right ? Your eagerness cost you dearly though Mr. Leguizamo ... &lt; br / &gt; &lt; br / &gt; The Pest follows in the tradition of any comedy film , and plays the &quot; race card &quot; , and more . No group is left out from being poked fun at . Blacks , Latinos , whites , Jews , Koreans , Germans , homosexuals , and the blind are among those singled out . Again here , things get too overboard , and too much tries to get spoofed in too little time . The resolution of the film takes all of five minutes to clear up and move back to normality . &lt; br / &gt; &lt; br / &gt; When you have a film , and you &apos;re going to bypass plot and reality for comedy &apos;s sake , just make sure it &apos;s funny , or all you have is 90 minutes of senseless film . Which would sum up Leguizamo &apos;s &quot; Pest &quot; quite nicely ...\n",
            "\n",
            "==> train.ful.tok.tsv <==\n",
            "NEG\tI have always wanted to see this because I love cheesy horror movies and with a title like this , I was sure &quot; The Incredible Melting Man &quot; would be a lot of fun . &lt; br / &gt; &lt; br / &gt; It really wasn &apos;t . I mean , the acting was entertainingly bad , the script contained some classic bad lines and the special effects looked like someone had sneezed all over the lead actor , so I should have loved it . Unfortunately it &apos;s really draggy between these highlights . I decided to watch the last half of the movie while doing my tax return . That &apos;s how boring this film is . &lt; br / &gt; &lt; br / &gt; Nevertheless , if you love bad movies you will enjoy the dramatic exit of the Fat Nurse , and the stellar acting of the guy who plays Dr. Ted . To be fair to the poor man , he does have to deliver some amazingly inept lines with straight face - like the conversation he has with his wife on tracking down the I M Man : &lt; br / &gt; &lt; br / &gt; &quot; I &apos;ll find him with a geiger counter . &quot; &quot; Is he radioactive ? &quot; &quot; Just a little bit . &quot; &lt; br / &gt; &lt; br / &gt; Yes , the plot has Dr. Ted wandering about trying to find a superstrong zombie killing machine armed only with what looks like a mini @-@ Dyson . He &apos;s a brave man . Unfortunately his plan fails when he finds a big lot of goop on a tree . &quot; Oh god - it &apos;s his ear ! &quot; says Dr. Ted to the audience . I &apos;m so glad he cleared that up . &lt; br / &gt; &lt; br / &gt; I realise I &apos;m making this movie sound rather fun . It would be if it were only 10 minutes long , but unfortunately it goes on and on , and the Incredible Melting Dude just dangles about making a sticky mess when he should be eating more people in my opinion . I think if you were truly stoned you would probably love it , just don &apos;t have pop @-@ tarts during the movie , because the lead actor really does resemble one near the end .\n",
            "NEG\tThis is one of the worst movies I &apos;ve seen in a long time . Not just the story , but the acting is shockingly bad . The dialog sounds like someone reading the news . &lt; br / &gt; &lt; br / &gt; This is rated as comedy / drama / romance , it &apos;s not of those things ! It &apos;s a little action , that &apos;s it . There &apos;s really NO comedy and drama at all . &lt; br / &gt; &lt; br / &gt; If you went to the cinema to see this I feel sorry for you . I would not recommend it at all . Pretty much anything else that you choose to look at will be better . This is pretty much a action / crime movie . The actions scenes sucked , and crime story part of it was very predictable . &lt; br / &gt; &lt; br / &gt; If you are not really interested in a good story , or good acting . And you simply want to look at a &apos; foreign &apos; film for the appeal of being foreign . Then this might be for you .\n",
            "Training\n",
            "  12500 NEG\n",
            "  12500 POS\n",
            "Test\n",
            "  12500 NEG\n",
            "  12500 POS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stpRaVxWGq_N",
        "colab_type": "text"
      },
      "source": [
        "### Create Validation Data\n",
        "\n",
        "*   To build machine learning models, we typically split data into train / validation / test.\n",
        "*   Validataion data is used to tune hyper-parameters and prevent overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muooqAHSFAnu",
        "colab_type": "code",
        "outputId": "98883463-fd4e-4aeb-d13e-57f80797925c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# shuffle ; first 2000 for validation, rest for training\n",
        "! shuf train.ful.tok.tsv > train.ful.tok.tsv.shuf \n",
        "! awk 'NR  <= 2000 ' train.ful.tok.tsv.shuf > valid.tok.tsv\n",
        "! awk 'NR  > 2000 ' train.ful.tok.tsv.shuf > train.tok.tsv\n",
        "! wc -l train.* valid.* test.*"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    25000 train.ful.tok.tsv\n",
            "    25000 train.ful.tok.tsv.shuf\n",
            "    23000 train.tok.tsv\n",
            "     2000 valid.tok.tsv\n",
            "    25000 test.tok.tsv\n",
            "   100000 total\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7Pzxa6rGgK0",
        "colab_type": "text"
      },
      "source": [
        "### Create Vocabulary\n",
        "\n",
        "Scan all training text, create token to idx mapping. \\\\\n",
        "Reserved types (tokens) are $<unk>, <pad>, <s>, </s>$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-N0mGjTKFXq2",
        "colab_type": "code",
        "outputId": "46c08e7a-529f-429c-c0df-2b229b56aad7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "from typing import List, Iterator, Set, Dict, Optional, Tuple\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "\n",
        "LABELS = ['NEG', 'POS']\n",
        "RESERVED = ['<pad>', '<unk>', '<s>', '</s>']\n",
        "# <s>, </s> are for future use during sequence modeling \n",
        "\n",
        "PAD_IDX = 0 \n",
        "UNK_IDX = 1\n",
        "MAX_TYPES = 40_000\n",
        "MAX_SEQ_LEN = 100\n",
        "\n",
        "class Vocab:\n",
        "  \"\"\" Mapper of words <--> index \"\"\"\n",
        "\n",
        "  def __init__(self, types):\n",
        "    # types is list of strings\n",
        "    assert isinstance(types, list)\n",
        "    assert isinstance(types[0], str)\n",
        "\n",
        "    self.idx2word = types\n",
        "    self.word2idx = {word: idx for idx, word in enumerate(types)}\n",
        "    assert len(self.idx2word) == len(self.word2idx)  # One-to-One\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.idx2word)\n",
        "  \n",
        "  def save(self, path: Path):\n",
        "    log.info(f'Saving vocab to {path}')\n",
        "    with path.open('w') as wr:\n",
        "      for word in self.idx2word:\n",
        "        wr.write(f'{word}\\n')\n",
        " \n",
        "  @staticmethod\n",
        "  def load(path):\n",
        "    log.info(f'loading vocab from {path}')\n",
        "    types = [line.strip() for line in path.open()]\n",
        "    for idx, tok in enumerate(RESERVED): # check reserved\n",
        "      assert types[idx] == tok\n",
        "    return Vocab(types)\n",
        "\n",
        "  @staticmethod\n",
        "  def from_text(corpus: Iterator[str], max_types: int,\n",
        "                             min_freq: int = 5):\n",
        "    \"\"\"\n",
        "    corpus: text corpus; iterator of strings\n",
        "    max_types: max size of vocabulary\n",
        "    min_freq: ignore word types that have fewer ferquency than this number\n",
        "    \"\"\"\n",
        "    log.info(\"building vocabulary; this might take some time\")\n",
        "    term_freqs = Counter(tok for line in corpus for tok in line.split())\n",
        "    for r in RESERVED:\n",
        "      if r in term_freqs:\n",
        "        log.warning(f'Found reserved word {r} in corpus')\n",
        "        del term_freqs[r]\n",
        "    term_freqs = list(term_freqs.items())\n",
        "    log.info(f\"Found {len(term_freqs)} types; given max_types={max_types}\")\n",
        "    term_freqs = {(t, f) for t, f in term_freqs if f >= min_freq}\n",
        "    log.info(f\"Found {len(term_freqs)} after dropping freq < {min_freq} terms\")\n",
        "    term_freqs = sorted(term_freqs, key=lambda x: x[1], reverse=True)\n",
        "    term_freqs = term_freqs[:max_types]\n",
        "    types = [t for t, f in term_freqs]\n",
        "    types = RESERVED + types   # prepend reserved words\n",
        "    return Vocab(types)\n",
        "\n",
        "\n",
        "train_file = Path('train.tok.tsv')\n",
        "valid_file = Path('valid.tok.tsv')\n",
        "vocab_file = Path('vocab.txt')\n",
        "\n",
        "if not vocab_file.exists():\n",
        "  train_corpus = (line.split('\\t')[1] for line in Path('train.tok.tsv').open())\n",
        "  vocab = Vocab.from_text(train_corpus, max_types=MAX_TYPES, min_freq=5)\n",
        "  vocab.save(vocab_file)\n",
        "else:\n",
        "  vocab = Vocab.load(vocab_file)\n",
        "\n",
        "log.info(f'Vocab has {len(vocab)} types')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:root:building vocabulary; this might take some time\n",
            "INFO:root:Found 100879 types; given max_types=40000\n",
            "INFO:root:Found 32722 after dropping freq < 5 terms\n",
            "INFO:root:Saving vocab to vocab.txt\n",
            "INFO:root:Vocab has 32726 types\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OaqB8RpXXLP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s1 = [5, 4, 9, 10]\n",
        "s2 = [6, 7, 8, 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPhKlKiDGNed",
        "colab_type": "code",
        "outputId": "30a2ce2c-8849-4234-e826-e99ce73dac6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "[(k,v) for k,v in vocab.word2idx.items()][:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('<pad>', 0),\n",
              " ('<unk>', 1),\n",
              " ('<s>', 2),\n",
              " ('</s>', 3),\n",
              " ('the', 4),\n",
              " (',', 5),\n",
              " ('.', 6),\n",
              " ('and', 7),\n",
              " ('a', 8),\n",
              " ('of', 9)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqhCCckeHICI",
        "colab_type": "code",
        "outputId": "ca057e4c-a9f8-48ba-b0a3-5032b05cb641",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(vocab.idx2word[0], vocab.idx2word[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<pad> <unk>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzQL71lpIRNM",
        "colab_type": "text"
      },
      "source": [
        "### Create [Dataloader](https://pytorch.org/docs/1.1.0/data.html)\n",
        "\n",
        "Batch computation \\\\\n",
        "Dataloader is generally a good practice in deep learning as data are usually very large."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "540fOFFHEwkP",
        "colab_type": "code",
        "outputId": "f543b8a9-8832-483e-f3a6-0ab8d42c13c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import copy\n",
        "import random\n",
        "\n",
        "Example = Tuple[List[int], int]\n",
        "\n",
        "class TextDataset:\n",
        "\n",
        "  def __init__(self, vocab: Vocab, path: Path, labels=LABELS, \n",
        "               has_label=True, max_len=MAX_SEQ_LEN):\n",
        "    self.vocab = vocab\n",
        "    self.label_map = Vocab(labels)\n",
        "    log.info(f'loading data from {path}')\n",
        "    # for simplicity, loading everything to memory; on large datasets this will cause OOM\n",
        "    fields = [line.split('\\t') for line in path.open()]\n",
        "\n",
        "    labels = [l for l, _ in fields]\n",
        "    text = [line.strip().split() for _, line in fields] # white space tokenizer\n",
        "    self.text = [toks[:max_len] for toks in text] # truncate long seqs\n",
        "\n",
        "    # Convert words to integer indexes\n",
        "    ys = [self.label_map.word2idx[l] for l in labels]\n",
        "    # words to index; out-of-vocab words are replaced with UNK\n",
        "    xs = [[self.vocab.word2idx.get(tok, UNK_IDX) for tok in tokss] \n",
        "                 for tokss in text]\n",
        "    self.data: List[Example] = list(zip(xs, ys))\n",
        "    label_stats = Counter(ys)\n",
        "    log.info(f\"Found {len(self.data)} records in {path}; labels={label_stats}\")\n",
        "\n",
        "  def as_batches(self, batch_size, shuffle=True):\n",
        "    data = self.data\n",
        "    if shuffle:\n",
        "      random.shuffle(data)\n",
        "    for i in range(0, len(data), batch_size): # i incrememt by batch_size\n",
        "      batch = data[i: i + batch_size]  # slice\n",
        "      yield self.batch_as_tensors(batch)\n",
        "  \n",
        "  @staticmethod\n",
        "  def batch_as_tensors(batch: List[Example]):\n",
        "    labels = [label for seq, label in batch]\n",
        "    seqs = [seq for seq, label in batch]\n",
        "    label_tensor = torch.tensor(labels, dtype=torch.long)\n",
        "    n_ex = len(batch)\n",
        "    max_len = max(len(seq) for seq in seqs)\n",
        "    seqs_tensor = torch.full(size=(n_ex, max_len), fill_value=PAD_IDX,\n",
        "                             dtype=torch.long)\n",
        "    for i, seq in enumerate(seqs):\n",
        "      seqs_tensor[i, 0:len(seq)] = torch.tensor(seq)\n",
        "    return seqs_tensor, label_tensor\n",
        "\n",
        "valid_data = TextDataset(vocab=vocab, path=valid_file)\n",
        "train_data = TextDataset(vocab=vocab, path=train_file)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:root:loading data from valid.tok.tsv\n",
            "INFO:root:Found 2000 records in valid.tok.tsv; labels=Counter({1: 1016, 0: 984})\n",
            "INFO:root:loading data from train.tok.tsv\n",
            "INFO:root:Found 23000 records in train.tok.tsv; labels=Counter({0: 11516, 1: 11484})\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkVwDlFfmoSQ",
        "colab_type": "code",
        "outputId": "41209122-efa8-4900-8130-aec78bc80c9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "for x in train_data.as_batches(batch_size=1):\n",
        "  print(x.shape)\n",
        "  print(x)\n",
        "  break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([[    1,  9623,    21,    20,  1329,    16,     4,  2142,     9,  7984,\n",
            "            20,    12,     8,  2452,  1457,    30,    10,   131,     6,   613,\n",
            "          9623,  1601,   232,     8,  4579,  3520,    58,   467,  4843,     6,\n",
            "            63,    12,     8,    27,    19,   209,    10,    41,  1003,   232,\n",
            "            70,   316,     5,   817,     7,  1078,   105,    10,    98,    16,\n",
            "             4,   304,  1218,   101,  2907,     6,    14,    15,    11,    13,\n",
            "            14,    15,    11,    13,    63,   134,     8,  8672,    35,     4,\n",
            "          1631,     1,    21,   329,    58,     4,   193,   803,     5,     4,\n",
            "           197, 19847,     4,   233,    10,     4,  1010,    72,   547,   798,\n",
            "           148,    47,   174,   304,   123,   987,     7,  3893,     6,  2731,\n",
            "            44,    12,  3301,    16,    44,   113,     5,    57,   206,    10,\n",
            "             4,   437,   431,     5,    33,    85,    44,   209,    10,   142,\n",
            "             4,   992,   246,     9,  3457,     6,     1,     5,     4,    30,\n",
            "            60,     8,  3772, 16472,   196,     6,   343,    37,   657,    10,\n",
            "            84,   115,   174,    98,   479,     7,     4,   227,    50,   479,\n",
            "            16,    82,   221, 30286,     6,    14,    15,    11,    13,    14,\n",
            "            15,    11,    13,    25,    78,   180,    44,   235,    59,     9,\n",
            "            47,   174,  4292,    16,     4,   493,    12,     4,   304,  5613,\n",
            "             9,  5614,   297,    44, 15424,     6,   137,   259,   883,    12,\n",
            "            38,   674,     4,   151,   303,    16,    75,   467,  4843,   188,\n",
            "          5182, 11984,     5,    26,    44,     1,    12,     4,   469,    28,\n",
            "           109,     9,     4,    98,   101,  2907,     6,    14,    15,    11,\n",
            "            13,    14,    15,    11,    13,    25,   135,    12,    65,   513,\n",
            "           613,  9623,    21,   514,     6,  1241, 11639,     5,  7369,  3562,\n",
            "             5,  3748,  3823,     5,   506, 23246,     5,  7355,     1,     5,\n",
            "             7,     4,   381,     9,     4,  1062,   201,     5,   111,   174,\n",
            "           123,   257,  1428,     6,    14,    15,    11,    13,    14,    15,\n",
            "            11,    13,   343,   727,   613,  9623,    65,  2274,     7, 13921,\n",
            "         18363,    29,    39,   437,   183,     6]]), tensor([1]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTg8Ka_nMsrB",
        "colab_type": "text"
      },
      "source": [
        "## [Word Embedding](https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa)\n",
        "\n",
        "Word2vec \\\\\n",
        "GloVe \\\\\n",
        "fastText \\\\\n",
        "ELMO \\\\\n",
        "BERT \\\\"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUyz7hqcMrn_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "class FNN_embedding(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, n_class, emb_dim=50, hid=10, dropout=0.1):\n",
        "    super().__init__() # call is necessary as per Pytorch API design\n",
        "    # embedding_dim same as n_class\n",
        "    self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
        "                                  embedding_dim=emb_dim, padding_idx=PAD_IDX)\n",
        "    \n",
        "    self.linear1 = nn.Linear(emb_dim, hid)\n",
        "    self.linear2 = nn.Linear(hid, n_class)\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "  def forward(self, seqs, log_probs=True):\n",
        "    \"\"\"Return log Probabilities\"\"\"\n",
        "    batch_size, max_len = seqs.shape\n",
        "    embs = self.embedding(seqs)  # embs[Batch x SeqLen x EmbDim]\n",
        "    embs = self.dropout(embs)\n",
        "    \n",
        "    mask = seqs == 0             # Masking padded time steps \n",
        "    mask = mask.view(batch_size, max_len, 1)   # unsqueeze(2)\n",
        "    embs.masked_fill(mask, value=0)\n",
        "    embs = embs.sum(dim=1)   # sum over all all steps in seq\n",
        "\n",
        "    hid_activated = torch.relu(self.linear1(embs)) # Non linear\n",
        "    scores = self.linear2(hid_activated)\n",
        "\n",
        "    if log_probs:\n",
        "      return torch.log_softmax(scores, dim=1)\n",
        "    else:\n",
        "      return torch.softmax(scores, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3r8UglMqK92H",
        "colab_type": "text"
      },
      "source": [
        "## Trainner Object --> train(), similar to what we just did in Part I.\n",
        "\n",
        "\n",
        "1.   Initilize model\n",
        "2.   Pick an optimizer and a loss function\n",
        "3.   Iterate through data N times\n",
        "4.   model.train()\n",
        "5.   batch calculation / prediction\n",
        "6.   optimizer.zero_grad()\n",
        "7.   Feed data into model and compute y_pred + loss\n",
        "8.   loss.backward()\n",
        "9.   evaluation --> model.eval()\n",
        "10.  go back to step 4  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gPmrJXXE54t",
        "colab_type": "code",
        "outputId": "9a049f96-4e6e-45ba-9ce3-ee2c5b1b489f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# Trainer Optimizer \n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train(model, optimizer, n_epochs, batch_size, train_data, valid_data, device=torch.device('cuda')):\n",
        "  log.info(f\"Moving model to {device}\")\n",
        "  model = model.to(device)   # move model to desired device \n",
        "  log.info(f\"Device for training {device}\")\n",
        "  losses = []\n",
        "  for epoch in range(n_epochs):\n",
        "    start = time.time()\n",
        "    num_toks = 0\n",
        "    train_loss = 0.\n",
        "    n_train_batches = 0\n",
        "\n",
        "    model.train() # switch to train mode \n",
        "    with tqdm(train_data.as_batches(batch_size=batch_size), leave=False) as data_bar:\n",
        "      for seqs, labels in data_bar:\n",
        "        # Move input to desired device\n",
        "        seqs = seqs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        num_toks += (seqs != PAD_IDX).sum().item()\n",
        "\n",
        "        log_probs = model(seqs)\n",
        "        loss = loss_func(log_probs, labels).sum() / len(seqs)\n",
        "        train_loss += loss.item()\n",
        "        \n",
        "        n_train_batches += 1\n",
        "\n",
        "        optimizer.zero_grad()         # clear grads\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        toks_speed = num_toks / (time.time() - start)\n",
        "        pbar_msg = f'Loss:{loss.item():.4f} {int(toks_speed)} toks/s'\n",
        "        data_bar.set_postfix_str(pbar_msg)\n",
        "      \n",
        "    # Run validation\n",
        "    with torch.no_grad():\n",
        "      model.eval() # switch to inference mode -- no grads, dropouts inactive\n",
        "      \n",
        "      val_loss = 0\n",
        "      n_val_batches = 0\n",
        "      for seqs, labels in valid_data.as_batches(batch_size=batch_size, shuffle=False):\n",
        "        # Move input to desired device\n",
        "        seqs = seqs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        log_probs = model(seqs)\n",
        "        loss = loss_func(log_probs, labels).sum() / len(seqs)\n",
        "        val_loss += loss.item() \n",
        "        n_val_batches += 1\n",
        "      \n",
        "    avg_train_loss = train_loss / n_train_batches\n",
        "    avg_val_loss = val_loss / n_val_batches\n",
        "    losses.append((epoch, avg_train_loss, avg_val_loss))\n",
        "    log.info(f\"Epoch {epoch} complete; Losses: Train={avg_train_loss:G} Valid={avg_val_loss:G}\")\n",
        "  return losses\n",
        "\n",
        "# Step 1\n",
        "model = FNN_embedding(vocab_size=len(vocab), n_class=len(LABELS))\n",
        "# Step 2\n",
        "loss_func = nn.NLLLoss(reduction='none')\n",
        "optimizer = optim.Adam(params=model.parameters())\n",
        "\n",
        "losses = train(model, optimizer, n_epochs=10, batch_size=100, \n",
        "               train_data=train_data, valid_data=valid_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:root:Moving model to cuda\n",
            "INFO:root:Device for training cuda\n",
            "INFO:root:Epoch 0 complete; Losses: Train=0.835289 Valid=0.668705\n",
            "INFO:root:Epoch 1 complete; Losses: Train=0.620528 Valid=0.587551\n",
            "INFO:root:Epoch 2 complete; Losses: Train=0.521444 Valid=0.503103\n",
            "INFO:root:Epoch 3 complete; Losses: Train=0.422191 Valid=0.436336\n",
            "INFO:root:Epoch 4 complete; Losses: Train=0.341995 Valid=0.398732\n",
            "INFO:root:Epoch 5 complete; Losses: Train=0.285016 Valid=0.376304\n",
            "INFO:root:Epoch 6 complete; Losses: Train=0.240073 Valid=0.366372\n",
            "INFO:root:Epoch 7 complete; Losses: Train=0.2073 Valid=0.36486\n",
            "INFO:root:Epoch 8 complete; Losses: Train=0.177015 Valid=0.362477\n",
            "INFO:root:Epoch 9 complete; Losses: Train=0.15255 Valid=0.366085\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flBVRvVvPUy5",
        "colab_type": "code",
        "outputId": "a4515ecf-65e6-4a52-bb7c-89cb466ba683",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "source": [
        "# Visualize the losses\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def viz_losses(losses):\n",
        "  epochs = [e for e, t, v in losses]\n",
        "  train_losses = [t for e, t, v in losses]\n",
        "  val_losses = [v for e, t, v in losses]\n",
        "  \n",
        "  plt.figure(figsize=(12, 8))\n",
        "  plt.plot(epochs, train_losses, color='blue', label = 'Training')\n",
        "  plt.plot(epochs, val_losses, color='red', label='Validation')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Average Loss')\n",
        "  plt.title('Training and Validation Losses')\n",
        "  plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
        "  plt.show()\n",
        "\n",
        "viz_losses(losses)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAAHwCAYAAACc4U/qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iPA4EwPoAML",
        "colab_type": "text"
      },
      "source": [
        "## Report model performance on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXqNKZFIneGa",
        "colab_type": "code",
        "outputId": "9a083c31-9baa-4e4b-af5f-01f7b91909b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "test_data = TextDataset(vocab=vocab, path=Path('test.tok.tsv'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:root:loading data from test.tok.tsv\n",
            "INFO:root:Found 25000 records in test.tok.tsv; labels=Counter({1: 12500, 0: 12500})\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMzgDQPTnruB",
        "colab_type": "code",
        "outputId": "70a45ce4-5f15-4e2d-8452-d48814f59245",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "from collections import defaultdict as ddict\n",
        "\n",
        "def confusion_matrix(model, test_data, batch_size=100, device=torch.device('cuda')):\n",
        "  matrix = ddict(lambda:ddict(int))\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    for seqs, truth in test_data.as_batches(batch_size=batch_size):\n",
        "      seqs = seqs.to(device)\n",
        "      truth = truth.to(device)\n",
        "      \n",
        "      log_probs = model(seqs)\n",
        "      top_class = log_probs.argmax(dim=1)\n",
        "      for tru, pred in zip(truth.tolist(), top_class.tolist()):\n",
        "        matrix[tru][pred] += 1\n",
        "\n",
        "  d = matrix\n",
        "  matrix = torch.zeros(len(d), len(d))\n",
        "  for tru, preds in d.items():\n",
        "    for pred, count in preds.items():\n",
        "      matrix[tru][pred] = count\n",
        "  return matrix  \n",
        "\n",
        "def report_performance(model):\n",
        "  cnf_mat = confusion_matrix(model, test_data, device=torch.device('cuda'))\n",
        "  print(\"Labels: \", LABELS)\n",
        "  print('Confusion Matrix:', cnf_mat)\n",
        "  accuracy = cnf_mat.diag().sum() / cnf_mat.sum()\n",
        "  print('Accuracy:', accuracy)\n",
        "  precision = cnf_mat.diag() / cnf_mat.sum(dim=0)\n",
        "  print('Precision:', precision)\n",
        "  recall = cnf_mat.diag() / cnf_mat.sum(dim=1)\n",
        "  print('Recall:', recall)\n",
        "  f1 = 2 * precision * recall / (precision + recall) \n",
        "  print(\"F1:\",  f1)\n",
        "\n",
        "report_performance(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Labels:  ['NEG', 'POS']\n",
            "Confusion Matrix: tensor([[10828.,  1672.],\n",
            "        [ 1761., 10739.]])\n",
            "Accuracy: tensor(0.8627)\n",
            "Precision: tensor([0.8601, 0.8653])\n",
            "Recall: tensor([0.8662, 0.8591])\n",
            "F1: tensor([0.8632, 0.8622])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhRvyEuHnu9I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}